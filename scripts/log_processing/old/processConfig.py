import glob
import sys
from pythonUtils.BinaryFiles import *
import numpy as np
from scipy import stats
from pythonUtils.VariableLoader import *
import time
import tracemalloc

""" 
    This file processes the data generated by a configuration of the multiscale experiment.
    It combines the results of all rats into a small set of file.
    Requirements: 
        Assumes existence of file mazeMetrics.csv in working directory
    Files generated:
        'run_times_lN.pickle' = episode . rat . steps 
            run times of all rats starting at location N. 
            When N=-1, results aggregate all locations
            
        'nrun_times.pickle' = episode . rat . steps
            Idem to previous but using normalized results
        
        'run_times_summary_lN.pickle' : episode . count . mean . std . min . 25% . 50% . 75% . max
            result of grabbing
        'nrun_times_summary_lN.pickle' : episode . count . mean . std . min . 25% . 50% . 75% . max
            Idem to previous but with normalized results

    
"""


def get_maze_metrics(base_folder, config, step_size):
    # find maze of the config
    configs = pd.read_csv(base_folder + 'configs.csv', sep='\t')
    configs = configs.drop(columns=['run_id']).drop_duplicates()
    configs = configs.set_index(['config'])
    config_maze = os.path.basename(configs.loc[config]['mazeFile'])

    # find maze metrics (calculate geometric mean and add it to DF)
    maze_metrics = pd.read_csv(base_folder + "mazes/mazeMetrics.csv")
    metric_gmean = maze_metrics.groupby('maze')['distance'].apply(stats.gmean).reset_index(name='distance')
    metric_gmean['pos'] = -1
    maze_metrics = maze_metrics.append(metric_gmean, ignore_index=True, sort=True)
    maze_metrics = maze_metrics.set_index(['maze', 'pos']).loc[config_maze]
    maze_metrics['minSteps'] = maze_metrics['distance'] / step_size
    return maze_metrics


def merge_data_from_all_rats(config_folder):
    # get number of rats and starting locations in this experiment:
    num_locations = np.unique(load_int_vector(config_folder + "r0-steps.bin")).size
    num_episodes = read_vector_size(config_folder + "r0-steps.bin") // num_locations
    num_rats = len(glob.glob(config_folder + "r*-V0.bin"))

    # create columns of the final data frame:
    rat_ids = np.repeat(np.arange(num_rats, dtype=np.uint8), num_locations * num_episodes)
    episode = np.tile(np.repeat(np.arange(num_episodes, dtype=np.uint16), num_locations), num_rats)
    locations = np.zeros(num_episodes * num_locations * num_rats, dtype=np.uint8)
    steps = np.zeros(num_episodes * num_locations * num_rats, dtype=np.uint16)

    # load info of each rat
    append_length = num_episodes * num_locations
    file_name = config_folder + "r{}-steps.bin"
    for rat_id in range(0, num_rats):
        r_start = append_length * rat_id
        r_end = append_length * (rat_id + 1)
        with open(file_name.format(rat_id), 'rb') as file:
            locations[r_start:r_end] = load_int_vector(file)
            steps[r_start:r_end] = load_int_vector(file)

    # create data frame from the columns
    return pd.DataFrame({'location': locations,
                         'episode': episode,
                         'rat': rat_ids,
                         'steps': steps
                         })


def process_and_save_runtimes(run_times, location, normalizer, config_folder):
    # save run times
    run_times_file_name = 'run_times_l{}.pickle'.format(location)
    run_times.to_pickle(config_folder + run_times_file_name)

    # create a summary
    # this takes the most amount of time: approx 1 min
    t = time.time()
    summary = run_times.drop(columns='rat')\
                       .groupby(['episode'])\
                       .describe()
    print('Summarizing: {}'.format(time.time() - t))
    summary.columns = summary.columns.droplevel()
    summary = summary.reset_index()
    # set data types to reduce memory
    summary['count']   = summary['count'].astype(np.uint8)
    summary.episode = summary.episode.astype(np.uint16)
    for col in summary.columns.drop(['count', 'episode']):
        m_type = np.uint16 if col not in ['mean', 'std'] and location != -1 else np.float32
        summary[col] = summary[col].astype(m_type)

    # save the summary
    summaries_name = 'run_times_summary_l{}.pickle'.format(location)
    summary.to_pickle(config_folder + summaries_name)

    # normalize and store nomalized results
    run_times.steps = (run_times.steps / normalizer).astype(np.float32)
    for col in ['mean', 'std', 'min', '25%', '50%', '75%', 'max']:
        summary[col] = (summary[col] / normalizer).astype(np.float32)

    run_times.to_pickle(config_folder + 'n' + run_times_file_name)
    summary.to_pickle(config_folder + 'n' + summaries_name)


def process_config(base_folder, config):
    # tracemalloc.start()
    t1 = time.time()
    base_folder = os.path.join(base_folder, '')
    config_folder = base_folder + config + '/'
    step_size = 0.08

    # get maze metrics:
    maze_metrics = get_maze_metrics(base_folder, config, step_size)

    # merge results from all rats
    all_run_times = merge_data_from_all_rats(config_folder)

    # divide results according to location and process them
    for location, run_times in all_run_times.groupby('location'):
        print('Processing location {}...'.format(location))
        run_times = run_times.drop(columns=['location']).reset_index(drop=True)
        normalizer = maze_metrics['minSteps'].iloc[location]
        process_and_save_runtimes(run_times, location, normalizer, config_folder)
        print()

    # aggregate rat run_times by location
    print('aggregating rats...')
    mean_run_times = all_run_times.groupby(['episode', 'rat'])['steps'] \
        .apply(stats.gmean) \
        .reset_index(name='steps')
    # reduce memory by casting to single and dual precision
    mean_run_times.episode = mean_run_times.episode.astype(np.uint16)
    mean_run_times.rat = mean_run_times.rat.astype(np.uint8)
    mean_run_times.steps = mean_run_times.steps.astype(np.float32)

    # process aggregated results
    print('processing aggregated results...')
    normalizer = maze_metrics['minSteps'].iloc[-1]
    process_and_save_runtimes(mean_run_times, -1, normalizer, config_folder)

    print('TOTAL TIME: {}'.format(time.time() - t1))
    # current, peak = tracemalloc.get_traced_memory()
    # tracemalloc.stop()
    # print('MEMORY: current {}MB, peak {}MB'.format(current / 10 ** 6, peak / 10 ** 6))

    # old code that might be usefule in the future:
    # config_geom_means = rat_geom_means.groupby(['episode'])['geom_mean'].agg({'geom_mean': ['mean', 'std']})
    # config_geom_means = config_geom_means.rename(columns={'mean': 'avg_geom_mean', 'std': 'std_geom_mean'})
    # config_geom_means = config_geom_means.reset_index()






def process_all_configs(base_folder):
    config_folders = get_list_of_configs(base_folder)
    # config_folders = ['c'+str(i) for i in range(266, 280)]
    for config in config_folders:
        print("processing: ", config)
        process_config(base_folder, config)


if __name__ == '__main__':
    # argv[1] = base folder
    # argv[2] = config
    if len(sys.argv) > 2:
        process_config(sys.argv[1], sys.argv[2])
    else:
        process_all_configs(sys.argv[1])
